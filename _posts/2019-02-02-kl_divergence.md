---
title: Kullback-Leibler Divergence
date: 2019-02-02
permalink: /posts/2019/02/kl-divergence/
tags:
  - machine_learning
---

**Topics:** Kullback-Leibler Divergence

One of the points on my long 'stuff-you-have-to-look-at' list is the Kullback-Leibler divergence. I finally took the time to take a detailed look at this topic. Due to problems with displaying the math formulas I decided to create a jupyter notebook with the blog post. You can find it [here](https://github.com/zotroneneis/resources/blob/master/KL_divergence.ipynb). 

There is also a **PDF version** of the explanations which has by far the nicest looks (e.g. all the equations are aligned). You can download it [here](http://alpopkes.com/files/kl_divergence.pdf).

Taking a detailed look at the KL divergence was definitely worth it! Next, I want to learn more about variational inference.

